{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "# Practical assignment Units 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:\n",
    "- **Joel Aviles Alcoba**, joel.aviles.alcoba@alumnos.upm.es\n",
    "- **Eduardo Cuadrado Camarero**, eduardo.cuadrado@alumnos.upm.es\n",
    "- **Victor de Marco Velasco**, v.dvelasco@alumnos.upm.es\n",
    "- **Sergio Ángel Serna Santamaría**, sa.serna@alumnos.upm.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83746,
     "status": "ok",
     "timestamp": 1741771661493,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "LjLS1WetFhCE",
    "outputId": "2ae8d582-dacf-4e1d-9e43-472e603aa3fb"
   },
   "outputs": [],
   "source": [
    "# !pip install gymnasium[classic-control]\n",
    "!pip install -q gymnasium[box2d] tensorflow matplotlib;\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added a seed for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we list all the hyperparameters that have been modified:\n",
    "- **LEARNING_RATE**: 0.001. The learning rate was adjusted to 0.001 because it proved to be more stable during training for the case of the LunarLander environment helping to prevent overshooting and ensuring smoother convergence.\n",
    "- **BATCH_SIZE**: 128. Another modification done was the batch size to 128. With this size we achieve a good balance between computational efficiency and stability in the learning process.\n",
    "- **MEMORY_SIZE**: 200_000. LunarLander trajectories are longer and more varied than CartPole\n",
    "- **WARMUP_STEPS**: 10_000. Allowing the agent to gather more diverse experiences before training starts.\n",
    "TODO: Add more hyperparameters if modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741771661498,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4N2yVwtuFlBu"
   },
   "outputs": [],
   "source": [
    "# Learning rate (more stable for LunarLander)\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Discount factor gamma\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Size of the replay memory (replay buffer)\n",
    "MEMORY_SIZE = 200_000\n",
    "\n",
    "# Warm-up: pasos mínimos antes de empezar a entrenar\n",
    "WARMUP_STEPS = 10_000\n",
    "\n",
    "# Entrenar cada N pasos de interacción (1 = cada paso)\n",
    "TRAIN_EVERY = 4\n",
    "\n",
    "# Exploración epsilon-greedy por PASOS (no por episodio)\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY_STEPS = 150_000  # lineal hasta EPSILON_END\n",
    "\n",
    "# Target network (soft update / Polyak)\n",
    "TAU = 0.005\n",
    "\n",
    "# Máximo número de episodios de entrenamiento (se para antes si alcanza el objetivo)\n",
    "MAX_EPISODES_FOR_TRAINING = 2000\n",
    "\n",
    "# Goal score a alcanzar durante entrenamiento (media de últimos episodios)\n",
    "TRAINING_GOAL = 200\n",
    "\n",
    "# Número de episodios usados para comprobar el objetivo durante entrenamiento\n",
    "EPISODES_TO_CHECK_TRAINING_GOAL = 50\n",
    "\n",
    "# Número de episodios para evaluar al final\n",
    "EPISODES_TO_EVALUATE_MODEL_PERFORMANCE = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoGaas6TAd6p"
   },
   "source": [
    "## Class ReplayMemory\n",
    "\n",
    "Memory of transitions for experience replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1741771661513,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "cQV7IfhFOoSh"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, number_of_observations):\n",
    "        # Replay buffer circular (ring buffer)\n",
    "        self.states = np.zeros((MEMORY_SIZE, number_of_observations), dtype=np.float32)\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations), dtype=np.float32)\n",
    "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.rewards = np.zeros(MEMORY_SIZE, dtype=np.float32)\n",
    "        self.dones = np.zeros(MEMORY_SIZE, dtype=np.float32)  # 1.0 si done, 0.0 si no\n",
    "\n",
    "        self.position = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_next, done):\n",
    "        self.states[self.position] = state\n",
    "        self.states_next[self.position] = state_next\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.dones[self.position] = 1.0 if done else 0.0\n",
    "\n",
    "        self.position = (self.position + 1) % MEMORY_SIZE\n",
    "        self.current_size = min(self.current_size + 1, MEMORY_SIZE)\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        batch = np.random.choice(self.current_size, batch_size, replace=False)\n",
    "        states = self.states[batch]\n",
    "        states_next = self.states_next[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        actions = self.actions[batch]\n",
    "        dones = self.dones[batch]\n",
    "        return states, actions, rewards, states_next, dones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gejKO0OYAsS4"
   },
   "source": [
    "## Class DQN\n",
    "\n",
    "Reinforcement learning agent with a Deep Q-Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    def __init__(self, shape, eps=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = eps\n",
    "\n",
    "    def update(self, x):\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        batch_mean = x.mean(axis=0)\n",
    "        batch_var = x.var(axis=0)\n",
    "        batch_count = x.shape[0] if x.ndim > 1 else 1\n",
    "\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + (delta**2) * self.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "\n",
    "        self.mean, self.var, self.count = new_mean, new_var, tot_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741771661514,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, number_of_observations, number_of_actions):\n",
    "        self.number_of_actions = int(number_of_actions)\n",
    "        self.number_of_observations = int(number_of_observations)\n",
    "\n",
    "        self.scores = []\n",
    "        self.memory = ReplayMemory(self.number_of_observations)\n",
    "\n",
    "        # Normalización online de observaciones\n",
    "        self.obs_rms = RunningMeanStd(self.number_of_observations)\n",
    "\n",
    "        # Online network (Dueling DQN)\n",
    "        self.model = self._build_dueling_network()\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self._build_dueling_network()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # Optimización estable\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=10.0)\n",
    "        self.loss_fn = keras.losses.Huber()\n",
    "\n",
    "    def _build_dueling_network(self):\n",
    "        inputs = keras.layers.Input(shape=(self.number_of_observations,), dtype=tf.float32)\n",
    "\n",
    "        x = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "        x = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        # Value stream V(s)\n",
    "        v = keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "        v = keras.layers.Dense(1, activation=\"linear\")(v)\n",
    "\n",
    "        # Advantage stream A(s,a)\n",
    "        a = keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "        a = keras.layers.Dense(self.number_of_actions, activation=\"linear\")(a)\n",
    "\n",
    "        # Mean advantage\n",
    "        a_mean = keras.layers.Lambda(\n",
    "            lambda t: tf.reduce_mean(t, axis=1, keepdims=True),\n",
    "            output_shape=(1,)\n",
    "        )(a)\n",
    "\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q = v + (a - a_mean)\n",
    "\n",
    "        return keras.Model(inputs=inputs, outputs=q)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # done debe ser \"terminal real\" (terminated), no truncated\n",
    "        state = np.asarray(state, dtype=np.float32)\n",
    "        next_state = np.asarray(next_state, dtype=np.float32)\n",
    "\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        # actualiza normalizador con estados reales observados\n",
    "        self.obs_rms.update(state)\n",
    "        self.obs_rms.update(next_state)\n",
    "\n",
    "    def _prep_state(self, state):\n",
    "        s = self.obs_rms.normalize(np.asarray(state, dtype=np.float32))\n",
    "        s = np.clip(s, -5, 5)\n",
    "        return s\n",
    "\n",
    "    def select(self, state, exploration_rate):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return random.randrange(self.number_of_actions)\n",
    "\n",
    "        s = self._prep_state(state)[None, :]  # (1, obs)\n",
    "        q_values = self.model(s, training=False).numpy()[0]\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def select_greedy_policy(self, state):\n",
    "        s = self._prep_state(state)[None, :]  # (1, obs)\n",
    "        q_values = self.model(s, training=False).numpy()[0]\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def _soft_update_target(self):\n",
    "        for target_var, var in zip(self.target_model.trainable_variables, self.model.trainable_variables):\n",
    "            target_var.assign((1.0 - TAU) * target_var + TAU * var)\n",
    "\n",
    "    def learn(self):\n",
    "        # No entrenes hasta tener suficiente buffer\n",
    "        if self.memory.current_size < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_memory(BATCH_SIZE)\n",
    "\n",
    "        # Normaliza states y next_states\n",
    "        states = np.clip(self.obs_rms.normalize(states), -5, 5).astype(np.float32)\n",
    "        next_states = np.clip(self.obs_rms.normalize(next_states), -5, 5).astype(np.float32)\n",
    "\n",
    "        # Tensores\n",
    "        states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states_t = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones_t = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # Double DQN target:\n",
    "        # a* = argmax_a Q_online(s', a)\n",
    "        next_q_online = self.model(next_states_t, training=False)\n",
    "        next_actions = tf.argmax(next_q_online, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Q_target(s', a*)\n",
    "        next_q_target = self.target_model(next_states_t, training=False)\n",
    "        idx = tf.stack([tf.range(BATCH_SIZE, dtype=tf.int32), next_actions], axis=1)\n",
    "        next_q = tf.gather_nd(next_q_target, idx)\n",
    "\n",
    "        targets = rewards_t + GAMMA * (1.0 - dones_t) * next_q\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states_t, training=True)\n",
    "            idx2 = tf.stack([tf.range(BATCH_SIZE, dtype=tf.int32), actions_t], axis=1)\n",
    "            q_sa = tf.gather_nd(q_values, idx2)\n",
    "            loss = self.loss_fn(targets, q_sa)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Soft update target network\n",
    "        self._soft_update_target()\n",
    "\n",
    "    def add_score(self, score):\n",
    "        self.scores.append(score)\n",
    "\n",
    "    def delete_scores(self):\n",
    "        self.scores = []\n",
    "\n",
    "    def average_score(self, number_of_episodes):\n",
    "        index = len(self.scores) - number_of_episodes\n",
    "        return float(np.mean(self.scores[max(0, index):]))\n",
    "\n",
    "    def display_scores_graphically(self):\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Training score evolution\")\n",
    "\n",
    "    def save_model(self, path=\"my_model.keras\"):\n",
    "        self.model.save(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time(time):\n",
    "    min = round(time // 60)\n",
    "    sec = round(time % 60)\n",
    "    if min > 0:\n",
    "        print(f\"{min} min {sec} seconds\")\n",
    "    else:\n",
    "        print(f\"{sec} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741771661518,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4LBloUSG0LmT"
   },
   "outputs": [],
   "source": [
    "def create_environment(render_mode=None):\n",
    "    # Create simulated environment\n",
    "    environment = gym.make(\"LunarLander-v3\", render_mode=render_mode)\n",
    "    number_of_observations = environment.observation_space.shape[0]\n",
    "    number_of_actions = environment.action_space.n\n",
    "    return environment, number_of_observations, number_of_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Program for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 91220,
     "status": "ok",
     "timestamp": 1741771752738,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "b7a65da4-bcb5-43f4-8eb4-1e86af4e97ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1: score -197.62 (epsilon: 1.000, avg(last 50): -197.62, buffer: 81, steps: 81)\n",
      "Episode    2: score -175.13 (epsilon: 1.000, avg(last 50): -186.37, buffer: 192, steps: 192)\n",
      "Episode    3: score -127.37 (epsilon: 1.000, avg(last 50): -166.71, buffer: 272, steps: 272)\n",
      "Episode    4: score  -60.63 (epsilon: 1.000, avg(last 50): -140.19, buffer: 338, steps: 338)\n",
      "Episode    5: score -393.71 (epsilon: 1.000, avg(last 50): -190.89, buffer: 465, steps: 465)\n",
      "Episode    6: score -195.62 (epsilon: 1.000, avg(last 50): -191.68, buffer: 541, steps: 541)\n",
      "Episode    7: score -315.52 (epsilon: 1.000, avg(last 50): -209.37, buffer: 654, steps: 654)\n",
      "Episode    8: score -108.86 (epsilon: 1.000, avg(last 50): -196.81, buffer: 718, steps: 718)\n",
      "Episode    9: score -201.87 (epsilon: 1.000, avg(last 50): -197.37, buffer: 839, steps: 839)\n",
      "Episode   10: score -118.69 (epsilon: 1.000, avg(last 50): -189.5, buffer: 901, steps: 901)\n",
      "Episode   11: score -105.41 (epsilon: 1.000, avg(last 50): -181.86, buffer: 1008, steps: 1008)\n",
      "Episode   12: score -135.68 (epsilon: 1.000, avg(last 50): -178.01, buffer: 1079, steps: 1079)\n",
      "Episode   13: score -229.99 (epsilon: 1.000, avg(last 50): -182.01, buffer: 1163, steps: 1163)\n",
      "Episode   14: score -207.68 (epsilon: 1.000, avg(last 50): -183.84, buffer: 1222, steps: 1222)\n",
      "Episode   15: score -451.96 (epsilon: 1.000, avg(last 50): -201.72, buffer: 1304, steps: 1304)\n",
      "Episode   16: score -144.75 (epsilon: 1.000, avg(last 50): -198.16, buffer: 1382, steps: 1382)\n",
      "Episode   17: score -127.73 (epsilon: 1.000, avg(last 50): -194.01, buffer: 1470, steps: 1470)\n",
      "Episode   18: score -143.82 (epsilon: 1.000, avg(last 50): -191.22, buffer: 1601, steps: 1601)\n",
      "Episode   19: score -275.81 (epsilon: 1.000, avg(last 50): -195.68, buffer: 1708, steps: 1708)\n",
      "Episode   20: score  -13.51 (epsilon: 1.000, avg(last 50): -186.57, buffer: 1794, steps: 1794)\n",
      "Episode   21: score -174.55 (epsilon: 1.000, avg(last 50): -186.0, buffer: 1877, steps: 1877)\n",
      "Episode   22: score -133.27 (epsilon: 1.000, avg(last 50): -183.6, buffer: 1935, steps: 1935)\n",
      "Episode   23: score -133.15 (epsilon: 1.000, avg(last 50): -181.41, buffer: 2026, steps: 2026)\n",
      "Episode   24: score -261.33 (epsilon: 1.000, avg(last 50): -184.74, buffer: 2131, steps: 2131)\n",
      "Episode   25: score -199.44 (epsilon: 1.000, avg(last 50): -185.32, buffer: 2260, steps: 2260)\n",
      "   >> Greedy eval (10 eps): -523.66\n",
      "Episode   26: score -433.82 (epsilon: 1.000, avg(last 50): -194.88, buffer: 2348, steps: 2348)\n",
      "Episode   27: score -287.15 (epsilon: 1.000, avg(last 50): -198.3, buffer: 2441, steps: 2441)\n",
      "Episode   28: score  -84.07 (epsilon: 1.000, avg(last 50): -194.22, buffer: 2510, steps: 2510)\n",
      "Episode   29: score   -7.45 (epsilon: 1.000, avg(last 50): -187.78, buffer: 2615, steps: 2615)\n",
      "Episode   30: score  -99.39 (epsilon: 1.000, avg(last 50): -184.83, buffer: 2705, steps: 2705)\n",
      "Episode   31: score -115.65 (epsilon: 1.000, avg(last 50): -182.6, buffer: 2819, steps: 2819)\n",
      "Episode   32: score -109.79 (epsilon: 1.000, avg(last 50): -180.33, buffer: 2934, steps: 2934)\n",
      "Episode   33: score  -217.5 (epsilon: 1.000, avg(last 50): -181.45, buffer: 3005, steps: 3005)\n",
      "Episode   34: score  -96.58 (epsilon: 1.000, avg(last 50): -178.96, buffer: 3083, steps: 3083)\n",
      "Episode   35: score  -121.3 (epsilon: 1.000, avg(last 50): -177.31, buffer: 3170, steps: 3170)\n",
      "Episode   36: score  -296.9 (epsilon: 1.000, avg(last 50): -180.63, buffer: 3295, steps: 3295)\n",
      "Episode   37: score -103.73 (epsilon: 1.000, avg(last 50): -178.55, buffer: 3351, steps: 3351)\n",
      "Episode   38: score -120.18 (epsilon: 1.000, avg(last 50): -177.02, buffer: 3421, steps: 3421)\n",
      "Episode   39: score -347.76 (epsilon: 1.000, avg(last 50): -181.39, buffer: 3534, steps: 3534)\n",
      "Episode   40: score -146.71 (epsilon: 1.000, avg(last 50): -180.53, buffer: 3589, steps: 3589)\n",
      "Episode   41: score -111.64 (epsilon: 1.000, avg(last 50): -178.85, buffer: 3675, steps: 3675)\n",
      "Episode   42: score -107.08 (epsilon: 1.000, avg(last 50): -177.14, buffer: 3750, steps: 3750)\n",
      "Episode   43: score -443.19 (epsilon: 1.000, avg(last 50): -183.33, buffer: 3835, steps: 3835)\n",
      "Episode   44: score -122.14 (epsilon: 1.000, avg(last 50): -181.94, buffer: 3914, steps: 3914)\n",
      "Episode   45: score -103.04 (epsilon: 1.000, avg(last 50): -180.18, buffer: 3974, steps: 3974)\n",
      "Episode   46: score -130.41 (epsilon: 1.000, avg(last 50): -179.1, buffer: 4048, steps: 4048)\n",
      "Episode   47: score -238.75 (epsilon: 1.000, avg(last 50): -180.37, buffer: 4135, steps: 4135)\n",
      "Episode   48: score  -111.0 (epsilon: 1.000, avg(last 50): -178.92, buffer: 4253, steps: 4253)\n",
      "Episode   49: score  -92.06 (epsilon: 1.000, avg(last 50): -177.15, buffer: 4351, steps: 4351)\n",
      "Episode   50: score  -88.08 (epsilon: 1.000, avg(last 50): -175.37, buffer: 4435, steps: 4435)\n",
      "   >> Greedy eval (10 eps): -646.49\n",
      "Episode   51: score -227.68 (epsilon: 1.000, avg(last 50): -175.97, buffer: 4535, steps: 4535)\n",
      "Episode   52: score  -29.62 (epsilon: 1.000, avg(last 50): -173.06, buffer: 4618, steps: 4618)\n",
      "Episode   53: score -293.99 (epsilon: 1.000, avg(last 50): -176.39, buffer: 4732, steps: 4732)\n",
      "Episode   54: score -353.73 (epsilon: 1.000, avg(last 50): -182.26, buffer: 4809, steps: 4809)\n",
      "Episode   55: score -111.32 (epsilon: 1.000, avg(last 50): -176.61, buffer: 4919, steps: 4919)\n",
      "Episode   56: score -211.78 (epsilon: 1.000, avg(last 50): -176.93, buffer: 5026, steps: 5026)\n",
      "Episode   57: score -231.09 (epsilon: 1.000, avg(last 50): -175.24, buffer: 5119, steps: 5119)\n",
      "Episode   58: score -270.35 (epsilon: 1.000, avg(last 50): -178.47, buffer: 5215, steps: 5215)\n",
      "Episode   59: score   -99.7 (epsilon: 1.000, avg(last 50): -176.43, buffer: 5271, steps: 5271)\n",
      "Episode   60: score  -131.0 (epsilon: 1.000, avg(last 50): -176.68, buffer: 5366, steps: 5366)\n",
      "Episode   61: score -115.72 (epsilon: 1.000, avg(last 50): -176.88, buffer: 5478, steps: 5478)\n",
      "Episode   62: score -119.72 (epsilon: 1.000, avg(last 50): -176.56, buffer: 5573, steps: 5573)\n",
      "Episode   63: score -117.82 (epsilon: 1.000, avg(last 50): -174.32, buffer: 5645, steps: 5645)\n",
      "Episode   64: score -153.07 (epsilon: 1.000, avg(last 50): -173.23, buffer: 5711, steps: 5711)\n",
      "Episode   65: score  -100.9 (epsilon: 1.000, avg(last 50): -166.21, buffer: 5817, steps: 5817)\n",
      "Episode   66: score  -51.43 (epsilon: 1.000, avg(last 50): -164.34, buffer: 5883, steps: 5883)\n",
      "Episode   67: score -123.41 (epsilon: 1.000, avg(last 50): -164.25, buffer: 5951, steps: 5951)\n",
      "Episode   68: score -457.85 (epsilon: 1.000, avg(last 50): -170.53, buffer: 6044, steps: 6044)\n",
      "Episode   69: score -141.62 (epsilon: 1.000, avg(last 50): -167.85, buffer: 6130, steps: 6130)\n",
      "Episode   70: score -494.09 (epsilon: 1.000, avg(last 50): -177.46, buffer: 6214, steps: 6214)\n",
      "Episode   71: score -100.61 (epsilon: 1.000, avg(last 50): -175.98, buffer: 6287, steps: 6287)\n",
      "Episode   72: score  -97.36 (epsilon: 1.000, avg(last 50): -175.26, buffer: 6357, steps: 6357)\n",
      "Episode   73: score  -93.03 (epsilon: 1.000, avg(last 50): -174.46, buffer: 6430, steps: 6430)\n",
      "Episode   74: score -224.45 (epsilon: 1.000, avg(last 50): -173.72, buffer: 6534, steps: 6534)\n",
      "Episode   75: score  -273.9 (epsilon: 1.000, avg(last 50): -175.21, buffer: 6633, steps: 6633)\n",
      "   >> Greedy eval (10 eps): -587.78\n",
      "Episode   76: score -212.84 (epsilon: 1.000, avg(last 50): -170.79, buffer: 6716, steps: 6716)\n",
      "Episode   77: score -142.13 (epsilon: 1.000, avg(last 50): -167.89, buffer: 6799, steps: 6799)\n",
      "Episode   78: score -197.93 (epsilon: 1.000, avg(last 50): -170.17, buffer: 6872, steps: 6872)\n",
      "Episode   79: score -247.85 (epsilon: 1.000, avg(last 50): -174.98, buffer: 6962, steps: 6962)\n",
      "Episode   80: score -131.02 (epsilon: 1.000, avg(last 50): -175.61, buffer: 7029, steps: 7029)\n",
      "Episode   81: score    4.37 (epsilon: 1.000, avg(last 50): -173.21, buffer: 7108, steps: 7108)\n",
      "Episode   82: score -180.89 (epsilon: 1.000, avg(last 50): -174.63, buffer: 7185, steps: 7185)\n",
      "Episode   83: score -261.96 (epsilon: 1.000, avg(last 50): -175.52, buffer: 7293, steps: 7293)\n",
      "Episode   84: score -102.54 (epsilon: 1.000, avg(last 50): -175.64, buffer: 7379, steps: 7379)\n",
      "Episode   85: score -197.04 (epsilon: 1.000, avg(last 50): -177.16, buffer: 7468, steps: 7468)\n",
      "Episode   86: score -311.18 (epsilon: 1.000, avg(last 50): -177.44, buffer: 7576, steps: 7576)\n",
      "Episode   87: score -238.39 (epsilon: 1.000, avg(last 50): -180.13, buffer: 7662, steps: 7662)\n",
      "Episode   88: score -113.38 (epsilon: 1.000, avg(last 50): -180.0, buffer: 7725, steps: 7725)\n",
      "Episode   89: score  -79.39 (epsilon: 1.000, avg(last 50): -174.63, buffer: 7789, steps: 7789)\n",
      "Episode   90: score -433.24 (epsilon: 1.000, avg(last 50): -180.36, buffer: 7897, steps: 7897)\n",
      "Episode   91: score -130.97 (epsilon: 1.000, avg(last 50): -180.75, buffer: 7991, steps: 7991)\n",
      "Episode   92: score -122.34 (epsilon: 1.000, avg(last 50): -181.05, buffer: 8072, steps: 8072)\n",
      "Episode   93: score -290.46 (epsilon: 1.000, avg(last 50): -178.0, buffer: 8177, steps: 8177)\n",
      "Episode   94: score -130.45 (epsilon: 1.000, avg(last 50): -178.16, buffer: 8261, steps: 8261)\n",
      "Episode   95: score   -83.2 (epsilon: 1.000, avg(last 50): -177.77, buffer: 8364, steps: 8364)\n",
      "Episode   96: score  -29.05 (epsilon: 1.000, avg(last 50): -175.74, buffer: 8442, steps: 8442)\n",
      "Episode   97: score  -269.0 (epsilon: 1.000, avg(last 50): -176.35, buffer: 8567, steps: 8567)\n",
      "Episode   98: score -248.26 (epsilon: 1.000, avg(last 50): -179.09, buffer: 8651, steps: 8651)\n",
      "Episode   99: score -416.28 (epsilon: 1.000, avg(last 50): -185.57, buffer: 8749, steps: 8749)\n",
      "Episode  100: score -277.79 (epsilon: 1.000, avg(last 50): -189.37, buffer: 8855, steps: 8855)\n",
      "   >> Greedy eval (10 eps): -450.26\n",
      "Episode  101: score  -204.0 (epsilon: 1.000, avg(last 50): -188.9, buffer: 8944, steps: 8944)\n",
      "Episode  102: score -158.28 (epsilon: 1.000, avg(last 50): -191.47, buffer: 9014, steps: 9014)\n",
      "Episode  103: score -337.93 (epsilon: 1.000, avg(last 50): -192.35, buffer: 9121, steps: 9121)\n",
      "Episode  104: score -187.34 (epsilon: 1.000, avg(last 50): -189.02, buffer: 9202, steps: 9202)\n",
      "Episode  105: score   48.49 (epsilon: 1.000, avg(last 50): -185.82, buffer: 9273, steps: 9273)\n",
      "Episode  106: score -153.27 (epsilon: 1.000, avg(last 50): -184.65, buffer: 9344, steps: 9344)\n",
      "Episode  107: score -339.92 (epsilon: 1.000, avg(last 50): -186.83, buffer: 9439, steps: 9439)\n",
      "Episode  108: score -270.58 (epsilon: 1.000, avg(last 50): -186.83, buffer: 9543, steps: 9543)\n",
      "Episode  109: score -141.93 (epsilon: 1.000, avg(last 50): -187.68, buffer: 9625, steps: 9625)\n",
      "Episode  110: score -301.98 (epsilon: 1.000, avg(last 50): -191.1, buffer: 9742, steps: 9742)\n",
      "Episode  111: score -171.74 (epsilon: 1.000, avg(last 50): -192.22, buffer: 9852, steps: 9852)\n",
      "Episode  112: score -313.35 (epsilon: 1.000, avg(last 50): -196.09, buffer: 9939, steps: 9939)\n",
      "Episode  113: score -362.32 (epsilon: 1.000, avg(last 50): -200.98, buffer: 10024, steps: 10024)\n",
      "Episode  114: score -173.78 (epsilon: 0.999, avg(last 50): -201.4, buffer: 10143, steps: 10143)\n",
      "Episode  115: score -305.66 (epsilon: 0.998, avg(last 50): -205.49, buffer: 10315, steps: 10315)\n",
      "Episode  116: score  -54.88 (epsilon: 0.997, avg(last 50): -205.56, buffer: 10388, steps: 10388)\n",
      "Episode  117: score -132.15 (epsilon: 0.997, avg(last 50): -205.73, buffer: 10473, steps: 10473)\n",
      "Episode  118: score -102.55 (epsilon: 0.996, avg(last 50): -198.63, buffer: 10566, steps: 10566)\n",
      "Episode  119: score -348.57 (epsilon: 0.996, avg(last 50): -202.77, buffer: 10657, steps: 10657)\n",
      "Episode  120: score -372.41 (epsilon: 0.995, avg(last 50): -200.33, buffer: 10776, steps: 10776)\n",
      "Episode  121: score -106.17 (epsilon: 0.994, avg(last 50): -200.45, buffer: 10863, steps: 10863)\n",
      "Episode  122: score -363.94 (epsilon: 0.994, avg(last 50): -205.78, buffer: 10943, steps: 10943)\n",
      "Episode  123: score -117.07 (epsilon: 0.993, avg(last 50): -206.26, buffer: 11033, steps: 11033)\n",
      "Episode  124: score -380.94 (epsilon: 0.993, avg(last 50): -209.39, buffer: 11136, steps: 11136)\n",
      "Episode  125: score  -79.37 (epsilon: 0.992, avg(last 50): -205.5, buffer: 11207, steps: 11207)\n",
      "   >> Greedy eval (10 eps): -169.16\n",
      "Episode  126: score -127.68 (epsilon: 0.992, avg(last 50): -203.79, buffer: 11278, steps: 11278)\n",
      "Episode  127: score  -72.97 (epsilon: 0.991, avg(last 50): -202.41, buffer: 11360, steps: 11360)\n",
      "Episode  128: score -336.23 (epsilon: 0.991, avg(last 50): -205.18, buffer: 11437, steps: 11437)\n",
      "Episode  129: score  -74.28 (epsilon: 0.990, avg(last 50): -201.71, buffer: 11512, steps: 11512)\n",
      "Episode  130: score  -78.84 (epsilon: 0.990, avg(last 50): -200.66, buffer: 11605, steps: 11605)\n",
      "Episode  131: score -136.88 (epsilon: 0.989, avg(last 50): -203.49, buffer: 11669, steps: 11669)\n",
      "Episode  132: score -316.19 (epsilon: 0.989, avg(last 50): -206.19, buffer: 11743, steps: 11743)\n",
      "Episode  133: score -169.17 (epsilon: 0.988, avg(last 50): -204.34, buffer: 11861, steps: 11861)\n",
      "Episode  134: score  -69.11 (epsilon: 0.987, avg(last 50): -203.67, buffer: 11929, steps: 11929)\n",
      "Episode  135: score -205.71 (epsilon: 0.987, avg(last 50): -203.84, buffer: 12005, steps: 12005)\n",
      "Episode  136: score -135.96 (epsilon: 0.986, avg(last 50): -200.34, buffer: 12132, steps: 12132)\n",
      "Episode  137: score -133.62 (epsilon: 0.986, avg(last 50): -198.24, buffer: 12215, steps: 12215)\n",
      "Episode  138: score -240.19 (epsilon: 0.985, avg(last 50): -200.78, buffer: 12329, steps: 12329)\n",
      "Episode  139: score -212.95 (epsilon: 0.984, avg(last 50): -203.45, buffer: 12443, steps: 12443)\n",
      "Episode  140: score  -83.48 (epsilon: 0.983, avg(last 50): -196.45, buffer: 12549, steps: 12549)\n",
      "Episode  141: score -230.12 (epsilon: 0.983, avg(last 50): -198.44, buffer: 12657, steps: 12657)\n",
      "Episode  142: score -109.59 (epsilon: 0.982, avg(last 50): -198.18, buffer: 12732, steps: 12732)\n",
      "Episode  143: score  -79.14 (epsilon: 0.982, avg(last 50): -193.96, buffer: 12797, steps: 12797)\n",
      "Episode  144: score -124.19 (epsilon: 0.981, avg(last 50): -193.83, buffer: 12885, steps: 12885)\n",
      "Episode  145: score -154.88 (epsilon: 0.980, avg(last 50): -195.26, buffer: 13025, steps: 13025)\n",
      "Episode  146: score -231.81 (epsilon: 0.979, avg(last 50): -199.32, buffer: 13148, steps: 13148)\n",
      "Episode  147: score  -188.3 (epsilon: 0.979, avg(last 50): -197.71, buffer: 13226, steps: 13226)\n",
      "Episode  148: score   11.98 (epsilon: 0.978, avg(last 50): -192.5, buffer: 13352, steps: 13352)\n",
      "Episode  149: score  -91.36 (epsilon: 0.977, avg(last 50): -186.0, buffer: 13463, steps: 13463)\n",
      "Episode  150: score -109.13 (epsilon: 0.977, avg(last 50): -182.63, buffer: 13539, steps: 13539)\n",
      "   >> Greedy eval (10 eps): -153.93\n",
      "Episode  151: score -120.36 (epsilon: 0.976, avg(last 50): -180.96, buffer: 13637, steps: 13637)\n",
      "Episode  152: score -285.52 (epsilon: 0.975, avg(last 50): -183.5, buffer: 13758, steps: 13758)\n",
      "Episode  153: score -300.36 (epsilon: 0.975, avg(last 50): -182.75, buffer: 13852, steps: 13852)\n",
      "Episode  154: score -168.37 (epsilon: 0.974, avg(last 50): -182.37, buffer: 13950, steps: 13950)\n",
      "Episode  155: score  -85.83 (epsilon: 0.974, avg(last 50): -185.06, buffer: 14014, steps: 14014)\n",
      "Episode  156: score -310.11 (epsilon: 0.973, avg(last 50): -188.19, buffer: 14145, steps: 14145)\n",
      "Episode  157: score -299.72 (epsilon: 0.972, avg(last 50): -187.39, buffer: 14259, steps: 14259)\n",
      "Episode  158: score -217.98 (epsilon: 0.971, avg(last 50): -186.34, buffer: 14368, steps: 14368)\n",
      "Episode  159: score -163.06 (epsilon: 0.971, avg(last 50): -186.76, buffer: 14427, steps: 14427)\n",
      "Episode  160: score -108.42 (epsilon: 0.971, avg(last 50): -182.89, buffer: 14508, steps: 14508)\n",
      "Episode  161: score -168.32 (epsilon: 0.970, avg(last 50): -182.82, buffer: 14620, steps: 14620)\n",
      "Episode  162: score -340.73 (epsilon: 0.969, avg(last 50): -183.37, buffer: 14703, steps: 14703)\n",
      "Episode  163: score  -59.77 (epsilon: 0.968, avg(last 50): -177.32, buffer: 14832, steps: 14832)\n",
      "Episode  164: score  -92.75 (epsilon: 0.968, avg(last 50): -175.7, buffer: 14904, steps: 14904)\n",
      "Episode  165: score  -78.33 (epsilon: 0.967, avg(last 50): -171.15, buffer: 14991, steps: 14991)\n",
      "Episode  166: score -131.34 (epsilon: 0.967, avg(last 50): -172.68, buffer: 15060, steps: 15060)\n",
      "Episode  167: score -125.96 (epsilon: 0.966, avg(last 50): -172.56, buffer: 15132, steps: 15132)\n",
      "Episode  168: score -307.33 (epsilon: 0.966, avg(last 50): -176.65, buffer: 15235, steps: 15235)\n",
      "Episode  169: score -155.46 (epsilon: 0.965, avg(last 50): -172.79, buffer: 15332, steps: 15332)\n",
      "Episode  170: score -123.06 (epsilon: 0.965, avg(last 50): -167.8, buffer: 15424, steps: 15424)\n",
      "Episode  171: score -303.16 (epsilon: 0.964, avg(last 50): -171.74, buffer: 15535, steps: 15535)\n",
      "Episode  172: score -181.43 (epsilon: 0.963, avg(last 50): -168.09, buffer: 15638, steps: 15638)\n",
      "Episode  173: score -165.92 (epsilon: 0.963, avg(last 50): -169.07, buffer: 15724, steps: 15724)\n",
      "Episode  174: score -202.21 (epsilon: 0.962, avg(last 50): -165.49, buffer: 15820, steps: 15820)\n",
      "Episode  175: score -166.94 (epsilon: 0.961, avg(last 50): -167.25, buffer: 15911, steps: 15911)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mbuffer: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(agent.memory.current_size) + \u001b[33m\"\u001b[39m\u001b[33m, steps: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(total_steps) + \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m25\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     greedy_mean = \u001b[43mevaluate_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   >> Greedy eval (10 eps): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgreedy_mean\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m greedy_mean > \u001b[38;5;28mgetattr\u001b[39m(agent, \u001b[33m\"\u001b[39m\u001b[33mbest_greedy\u001b[39m\u001b[33m\"\u001b[39m, -\u001b[32m1e9\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mevaluate_greedy\u001b[39m\u001b[34m(agent, env, n_episodes)\u001b[39m\n\u001b[32m     21\u001b[39m total = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     a = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     s, r, terminated, truncated, _ = env.step(a)\n\u001b[32m     25\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mDQN.select_greedy_policy\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_greedy_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[32m     74\u001b[39m     s = \u001b[38;5;28mself\u001b[39m._prep_state(state)[\u001b[38;5;28;01mNone\u001b[39;00m, :]  \u001b[38;5;66;03m# (1, obs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.numpy()[\u001b[32m0\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(np.argmax(q_values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:953\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    957\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:183\u001b[39m, in \u001b[36mFunctional.call\u001b[39m\u001b[34m(self, inputs, training, mask, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    182\u001b[39m             backend.set_keras_mask(x, mask)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\ops\\function.py:206\u001b[39m, in \u001b[36mFunction._run_through_graph\u001b[39m\u001b[34m(self, inputs, operation_fn, call_fn)\u001b[39m\n\u001b[32m    204\u001b[39m     operation = \u001b[38;5;28mself\u001b[39m._get_operation_for_node(node)\n\u001b[32m    205\u001b[39m     op = operation_fn(operation)\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     outputs = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node.outputs, tree.flatten(outputs)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:647\u001b[39m, in \u001b[36moperation_fn.<locals>.call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    642\u001b[39m         name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(operation, \u001b[33m\"\u001b[39m\u001b[33m_call_context_args\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    643\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    644\u001b[39m     ):\n\u001b[32m    645\u001b[39m         kwargs[name] = value\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:953\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    957\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py:59\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     55\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     56\u001b[39m         call_fn,\n\u001b[32m     57\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:203\u001b[39m, in \u001b[36mDense.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m         x = ops.add(x, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\ops\\numpy.py:4689\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m   4687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x1, x2)):\n\u001b[32m   4688\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Matmul().symbolic_call(x1, x2)\n\u001b[32m-> \u001b[39m\u001b[32m4689\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:520\u001b[39m, in \u001b[36mmatmul\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    518\u001b[39m     output_type = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    519\u001b[39m x1 = tf.cast(x1, compute_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m x2 = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_combined_batch_dimensions\u001b[39m(a, b, output_shape, fn_3d):\n\u001b[32m    523\u001b[39m     a_sparse = \u001b[38;5;28misinstance\u001b[39m(a, tf.SparseTensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1266\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1012\u001b[39m, in \u001b[36mcast\u001b[39m\u001b[34m(x, dtype, name)\u001b[39m\n\u001b[32m   1006\u001b[39m   x = indexed_slices.IndexedSlices(values_cast, x.indices, x.dense_shape)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1008\u001b[39m   \u001b[38;5;66;03m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[32m   1009\u001b[39m   \u001b[38;5;66;03m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[32m   1010\u001b[39m   \u001b[38;5;66;03m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[32m   1011\u001b[39m   \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m   x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m x.dtype.is_complex \u001b[38;5;129;01mand\u001b[39;00m base_type.is_floating:\n\u001b[32m   1014\u001b[39m     logging.warn(\n\u001b[32m   1015\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are casting an input of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1016\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mincompatible dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.  This will \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdiscard the imaginary part and may not be what you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mintended.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[39m, in \u001b[36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:757\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    756\u001b[39m preferred_dtype = preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:209\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    207\u001b[39m overload = \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33m__tf_tensor__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[32m    212\u001b[39m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[32m    214\u001b[39m   ret = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:84\u001b[39m, in \u001b[36mVariable.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1266\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001b[39m, in \u001b[36mconvert_to_tensor_v2_with_dispatch\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@tf_export\u001b[39m.tf_export(\u001b[33m\"\u001b[39m\u001b[33mconvert_to_tensor\u001b[39m\u001b[33m\"\u001b[39m, v1=[])\n\u001b[32m     97\u001b[39m \u001b[38;5;129m@dispatch\u001b[39m.add_dispatch_support\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[32m     99\u001b[39m     value, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m ) -> tensor_lib.Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[32m    102\u001b[39m \n\u001b[32m    103\u001b[39m \u001b[33;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001b[39m, in \u001b[36mconvert_to_tensor_v2\u001b[39m\u001b[34m(value, dtype, dtype_hint, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_hint\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2378\u001b[39m, in \u001b[36m_dense_var_to_tensor\u001b[39m\u001b[34m(var, dtype, name, as_ref)\u001b[39m\n\u001b[32m   2377\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dense_var_to_tensor\u001b[39m(var, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m, as_ref=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1624\u001b[39m, in \u001b[36mBaseResourceVariable._dense_var_to_tensor\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1622\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_value().op.inputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:658\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:843\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    841\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    846\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    847\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    848\u001b[39m   record.record_operation(\n\u001b[32m    849\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    850\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    851\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat.forward_compatible(\u001b[32m2022\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m):\n\u001b[32m    832\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m result = \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m._dtype, \u001b[38;5;28mself\u001b[39m.handle, result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:534\u001b[39m, in \u001b[36mread_variable_op\u001b[39m\u001b[34m(resource, dtype, name)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    533\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadVariableOp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    537\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "environment, number_of_observations, number_of_actions = create_environment()\n",
    "agent = DQN(number_of_observations, number_of_actions)\n",
    "\n",
    "episode = 0\n",
    "start_time = time.perf_counter()\n",
    "total_steps = 0\n",
    "goal_reached = False\n",
    "\n",
    "def epsilon_by_step(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return 1.0\n",
    "    step2 = step - WARMUP_STEPS\n",
    "    decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS\n",
    "    return max(EPSILON_END, EPSILON_START - decay * step2)\n",
    "\n",
    "def evaluate_greedy(agent, env, n_episodes=10):\n",
    "    scores = []\n",
    "    for i in range(n_episodes):\n",
    "        s, _ = env.reset()  # sin seed fijo\n",
    "        done = False\n",
    "        total = 0.0\n",
    "        while not done:\n",
    "            a = agent.select_greedy_policy(s)\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            total += r\n",
    "        scores.append(total)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "while (episode < MAX_EPISODES_FOR_TRAINING) and not(goal_reached):\n",
    "    episode += 1\n",
    "    score = 0.0\n",
    "    state, info = environment.reset(seed=SEED + episode)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        exploration_rate = epsilon_by_step(total_steps)\n",
    "        action = agent.select(state, exploration_rate)\n",
    "\n",
    "        state_next, reward, terminated, truncated, info = environment.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        done_for_target = terminated\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        agent.remember(state, action, reward, state_next, done_for_target)\n",
    "        score += reward\n",
    "\n",
    "        if (total_steps >= WARMUP_STEPS) and (total_steps % TRAIN_EVERY == 0):\n",
    "            agent.learn()\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "    agent.add_score(score)\n",
    "    average_score = agent.average_score(EPISODES_TO_CHECK_TRAINING_GOAL)\n",
    "\n",
    "    print(\"Episode {0:>4}: \".format(episode), end='')\n",
    "    print(\"score {0:>7} \".format(round(score, 2)), end='')\n",
    "    print(\"(epsilon: %.3f, \" % epsilon_by_step(total_steps), end='')\n",
    "    print(\"avg(last {0}): {1:>6}, \".format(EPISODES_TO_CHECK_TRAINING_GOAL, round(average_score, 2)), end='')\n",
    "    print(\"buffer: \" + str(agent.memory.current_size) + \", steps: \" + str(total_steps) + \")\")\n",
    "\n",
    "\n",
    "    if episode % 25 == 0:\n",
    "        greedy_mean = evaluate_greedy(agent, environment, n_episodes=10)\n",
    "        print(f\"   >> Greedy eval (10 eps): {greedy_mean:.2f}\")\n",
    "\n",
    "        if greedy_mean > getattr(agent, \"best_greedy\", -1e9):\n",
    "            agent.best_greedy = greedy_mean\n",
    "            agent.model.save(\"best_model.keras\")\n",
    "\n",
    "        # criterio de parada REAL (el que importa para la práctica)\n",
    "        if greedy_mean >= TRAINING_GOAL:\n",
    "            print(\"✅ Stop: greedy_mean >= 200\")\n",
    "            goal_reached = True\n",
    "\n",
    "\n",
    "    if (episode >= EPISODES_TO_CHECK_TRAINING_GOAL) and (average_score >= TRAINING_GOAL):\n",
    "        goal_reached = True\n",
    "\n",
    "print(\"Time for training: \", end='')\n",
    "print_time(time.perf_counter() - start_time)\n",
    "print(\"Total steps: \", total_steps)\n",
    "print(\"Score (average last episodes):\", round(average_score, 2))\n",
    "print(\"Score (max):\", round(max(agent.scores), 2))\n",
    "\n",
    "agent.display_scores_graphically()\n",
    "agent.model.save(\"last_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSteJT6ULQVG"
   },
   "source": [
    "\n",
    "## Testing program\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41809,
     "status": "ok",
     "timestamp": 1741771908239,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "gIP0LQHGLZPj",
    "outputId": "7e94342e-00d7-408b-90d2-61066f33d267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1: score 266 \n",
      "Episode   2: score 285 \n",
      "Episode   3: score 262 \n",
      "Episode   4: score 273 \n",
      "Episode   5: score 258 \n",
      "Episode   6: score 270 \n",
      "Episode   7: score 275 \n",
      "Episode   8: score 270 \n",
      "Episode   9: score 221 \n",
      "Episode  10: score 244 \n",
      "Episode  11: score 260 \n",
      "Episode  12: score 275 \n",
      "Episode  13: score 262 \n",
      "Episode  14: score 288 \n",
      "Episode  15: score 301 \n",
      "Episode  16: score 278 \n",
      "Episode  17: score 283 \n",
      "Episode  18: score 251 \n",
      "Episode  19: score 217 \n",
      "Episode  20: score 163 \n",
      "Episode  21: score 243 \n",
      "Episode  22: score 250 \n",
      "Episode  23: score 109 \n",
      "Episode  24: score 272 \n",
      "Episode  25: score 272 \n",
      "Episode  26: score 311 \n",
      "Episode  27: score 237 \n",
      "Episode  28: score 279 \n",
      "Episode  29: score 247 \n",
      "Episode  30: score 125 \n",
      "Time for testing: 49 seconds\n",
      "Score (average): 252\n",
      "Score (max): 311\n"
     ]
    }
   ],
   "source": [
    "agent.delete_scores()\n",
    "episode = 0\n",
    "start_time = time.perf_counter()\n",
    "while (episode < EPISODES_TO_EVALUATE_MODEL_PERFORMANCE):\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state, info = environment.reset()\n",
    "    end_episode = False\n",
    "    while not(end_episode):\n",
    "        # Select an action for the current state\n",
    "        action = agent.select_greedy_policy(state)\n",
    "\n",
    "        # Execute the action in the environment\n",
    "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # Detect end of episode and print\n",
    "        if terminal_state or truncated:\n",
    "            agent.add_score(score)\n",
    "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
    "            print(\"score {0:>3} \\n\".format(round(score)), end = '')\n",
    "            end_episode = True\n",
    "        else:\n",
    "            state = state_next\n",
    "\n",
    "print(\"Time for testing: \", end = '')\n",
    "print_time(time.perf_counter() - start_time)\n",
    "print(\"Score (average):\", round(np.mean(agent.scores)))\n",
    "print(\"Score (max):\", round(max(agent.scores)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
