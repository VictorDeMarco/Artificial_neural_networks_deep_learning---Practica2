{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TT3LPGc5PuK"
   },
   "source": [
    "# Practical assignment Units 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors:\n",
    "- **Joel Aviles Alcoba**, joel.aviles.alcoba@alumnos.upm.es\n",
    "- **Eduardo Cuadrado Camarero**, eduardo.cuadrado@alumnos.upm.es\n",
    "- **Victor de Marco Velasco**, v.dvelasco@alumnos.upm.es\n",
    "- **Sergio Ángel Serna Santamaría**, sa.serna@alumnos.upm.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83746,
     "status": "ok",
     "timestamp": 1741771661493,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "LjLS1WetFhCE",
    "outputId": "2ae8d582-dacf-4e1d-9e43-472e603aa3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: gymnasium[box2d] in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (2.4.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d==2.3.10 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (2.3.10)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from gymnasium[box2d]) (4.4.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (6.33.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (3.13.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sergi\\documents\\github\\artificial_neural_networks_deep_learning---practica2\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q gymnasium[box2d] tensorflow matplotlib;\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBrRuhN1AQ-s"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741771661498,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4N2yVwtuFlBu"
   },
   "outputs": [],
   "source": [
    "# Learning rate (more stable for LunarLander)\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Discount factor gamma\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Size of the replay memory (replay buffer)\n",
    "MEMORY_SIZE = 200_000\n",
    "\n",
    "# Warm-up: pasos mínimos antes de empezar a entrenar\n",
    "WARMUP_STEPS = 10_000\n",
    "\n",
    "# Entrenar cada N pasos de interacción (1 = cada paso)\n",
    "TRAIN_EVERY = 4\n",
    "\n",
    "# Exploración epsilon-greedy por PASOS (no por episodio)\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY_STEPS = 150_000  # lineal hasta EPSILON_END\n",
    "\n",
    "# Target network (soft update / Polyak)\n",
    "TAU = 0.005\n",
    "\n",
    "# Máximo número de episodios de entrenamiento (se para antes si alcanza el objetivo)\n",
    "MAX_EPISODES_FOR_TRAINING = 2000\n",
    "\n",
    "# Goal score a alcanzar durante entrenamiento (media de últimos episodios)\n",
    "TRAINING_GOAL = 200\n",
    "\n",
    "# Número de episodios usados para comprobar el objetivo durante entrenamiento\n",
    "EPISODES_TO_CHECK_TRAINING_GOAL = 50\n",
    "\n",
    "# Número de episodios para evaluar al final\n",
    "EPISODES_TO_EVALUATE_MODEL_PERFORMANCE = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoGaas6TAd6p"
   },
   "source": [
    "## Class ReplayMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internal structure of the `ReplayMemory` class remains unchanged from the previous implementation for the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1741771661513,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "cQV7IfhFOoSh"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, number_of_observations):\n",
    "        # Create replay memory\n",
    "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.rewards = np.zeros(MEMORY_SIZE)\n",
    "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
    "        self.current_size = 0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
    "        # Store a transition (s,a,r,s') in the replay memory\n",
    "        i = self.current_size\n",
    "        self.states[i] = state\n",
    "        self.states_next[i] = state_next\n",
    "        self.actions[i] = action\n",
    "        self.rewards[i] = reward\n",
    "        self.terminal_states[i] = terminal_state\n",
    "        self.current_size = i + 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        # Generate a sample of transitions from the replay memory\n",
    "        batch = np.random.choice(self.current_size, batch_size)\n",
    "        states = self.states[batch]\n",
    "        states_next = self.states_next[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        actions = self.actions[batch]\n",
    "        terminal_states = self.terminal_states[batch]\n",
    "        return states, actions, rewards, states_next, terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gejKO0OYAsS4"
   },
   "source": [
    "## Class DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented `RunningMeanStd` to dynamically normalize observations to $\\mathcal{N}(0,1)$. This standardization ensures consistently scaled inputs, preventing gradient issues associated with raw environment ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    def __init__(self, shape, eps=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = eps\n",
    "\n",
    "    def update(self, x):\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        batch_mean = x.mean(axis=0)\n",
    "        batch_var = x.var(axis=0)\n",
    "        batch_count = x.shape[0] if x.ndim > 1 else 1\n",
    "\n",
    "        delta = batch_mean - self.mean\n",
    "        tot_count = self.count + batch_count\n",
    "        new_mean = self.mean + delta * batch_count / tot_count\n",
    "\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + (delta**2) * self.count * batch_count / tot_count\n",
    "        new_var = M2 / tot_count\n",
    "\n",
    "        self.mean, self.var, self.count = new_mean, new_var, tot_count\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DQN` class upgrades to a **Dueling Architecture** with **Double Q-Learning** to decouple action selection from evaluation and reduce maximization bias. We integrated **Soft Target Updates** via Polyak averaging and employed **Huber Loss** with **Gradient Clipping** to handle outliers and exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741771661514,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "NZ6P4Gj0FtnU"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, number_of_observations, number_of_actions):\n",
    "        self.number_of_actions = int(number_of_actions)\n",
    "        self.number_of_observations = int(number_of_observations)\n",
    "\n",
    "        self.scores = []\n",
    "        self.memory = ReplayMemory(self.number_of_observations)\n",
    "\n",
    "        # Normalización online de observaciones\n",
    "        self.obs_rms = RunningMeanStd(self.number_of_observations)\n",
    "\n",
    "        # Online network (Dueling DQN)\n",
    "        self.model = self._build_dueling_network()\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self._build_dueling_network()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # Optimización estable\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=10.0)\n",
    "        self.loss_fn = keras.losses.Huber()\n",
    "\n",
    "    def _build_dueling_network(self):\n",
    "        inputs = keras.layers.Input(shape=(self.number_of_observations,), dtype=tf.float32)\n",
    "\n",
    "        x = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(inputs)\n",
    "        x = keras.layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "        # Value stream V(s)\n",
    "        v = keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "        v = keras.layers.Dense(1, activation=\"linear\")(v)\n",
    "\n",
    "        # Advantage stream A(s,a)\n",
    "        a = keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "        a = keras.layers.Dense(self.number_of_actions, activation=\"linear\")(a)\n",
    "\n",
    "        # Mean advantage\n",
    "        a_mean = keras.layers.Lambda(\n",
    "            lambda t: tf.reduce_mean(t, axis=1, keepdims=True),\n",
    "            output_shape=(1,)\n",
    "        )(a)\n",
    "\n",
    "        # Q(s,a) = V(s) + (A(s,a) - mean(A))\n",
    "        q = v + (a - a_mean)\n",
    "\n",
    "        return keras.Model(inputs=inputs, outputs=q)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # done debe ser \"terminal real\" (terminated), no truncated\n",
    "        state = np.asarray(state, dtype=np.float32)\n",
    "        next_state = np.asarray(next_state, dtype=np.float32)\n",
    "\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        # actualiza normalizador con estados reales observados\n",
    "        self.obs_rms.update(state)\n",
    "        self.obs_rms.update(next_state)\n",
    "\n",
    "    def _prep_state(self, state):\n",
    "        s = self.obs_rms.normalize(np.asarray(state, dtype=np.float32))\n",
    "        s = np.clip(s, -5, 5)\n",
    "        return s\n",
    "\n",
    "    def select(self, state, exploration_rate):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < exploration_rate:\n",
    "            return random.randrange(self.number_of_actions)\n",
    "\n",
    "        s = self._prep_state(state)[None, :]  # (1, obs)\n",
    "        q_values = self.model(s, training=False).numpy()[0]\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def select_greedy_policy(self, state):\n",
    "        s = self._prep_state(state)[None, :]  # (1, obs)\n",
    "        q_values = self.model(s, training=False).numpy()[0]\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def _soft_update_target(self):\n",
    "        for target_var, var in zip(self.target_model.trainable_variables, self.model.trainable_variables):\n",
    "            target_var.assign((1.0 - TAU) * target_var + TAU * var)\n",
    "\n",
    "    def learn(self):\n",
    "        # No entrenes hasta tener suficiente buffer\n",
    "        if self.memory.current_size < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_memory(BATCH_SIZE)\n",
    "\n",
    "        # Normaliza states y next_states\n",
    "        states = np.clip(self.obs_rms.normalize(states), -5, 5).astype(np.float32)\n",
    "        next_states = np.clip(self.obs_rms.normalize(next_states), -5, 5).astype(np.float32)\n",
    "\n",
    "        # Tensores\n",
    "        states_t = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states_t = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions_t = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards_t = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones_t = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        # Double DQN target:\n",
    "        # a* = argmax_a Q_online(s', a)\n",
    "        next_q_online = self.model(next_states_t, training=False)\n",
    "        next_actions = tf.argmax(next_q_online, axis=1, output_type=tf.int32)\n",
    "\n",
    "        # Q_target(s', a*)\n",
    "        next_q_target = self.target_model(next_states_t, training=False)\n",
    "        idx = tf.stack([tf.range(BATCH_SIZE, dtype=tf.int32), next_actions], axis=1)\n",
    "        next_q = tf.gather_nd(next_q_target, idx)\n",
    "\n",
    "        targets = rewards_t + GAMMA * (1.0 - dones_t) * next_q\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states_t, training=True)\n",
    "            idx2 = tf.stack([tf.range(BATCH_SIZE, dtype=tf.int32), actions_t], axis=1)\n",
    "            q_sa = tf.gather_nd(q_values, idx2)\n",
    "            loss = self.loss_fn(targets, q_sa)\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Soft update target network\n",
    "        self._soft_update_target()\n",
    "\n",
    "    def add_score(self, score):\n",
    "        self.scores.append(score)\n",
    "\n",
    "    def delete_scores(self):\n",
    "        self.scores = []\n",
    "\n",
    "    def average_score(self, number_of_episodes):\n",
    "        index = len(self.scores) - number_of_episodes\n",
    "        return float(np.mean(self.scores[max(0, index):]))\n",
    "\n",
    "    def display_scores_graphically(self):\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(\"Training score evolution\")\n",
    "\n",
    "    def save_model(self, path=\"my_model.keras\"):\n",
    "        self.model.save(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reamins unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time(time):\n",
    "    min = round(time // 60)\n",
    "    sec = round(time % 60)\n",
    "    if min > 0:\n",
    "        print(f\"{min} min {sec} seconds\")\n",
    "    else:\n",
    "        print(f\"{sec} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-YSpziT0K9I"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reamins unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1741771661518,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "4LBloUSG0LmT"
   },
   "outputs": [],
   "source": [
    "def create_environment(render_mode=None):\n",
    "    # Create simulated environment\n",
    "    environment = gym.make(\"LunarLander-v3\", render_mode=render_mode)\n",
    "    number_of_observations = environment.observation_space.shape[0]\n",
    "    number_of_actions = environment.action_space.n\n",
    "    return environment, number_of_observations, number_of_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbw6blhDcsJ"
   },
   "source": [
    "## Program for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified the training loop to use a **Step-based Exploration** strategy (`epsilon_by_step`), which decays epsilon based on global steps and includes a warmup phase for initial data collection. Also, we restricted the **Training Frequency** to execute `agent.learn()` only every 4 steps (`TRAIN_EVERY`) to decorrelate gradient updates and improve stability. Additionally, we integrated a **Periodic Greedy Evaluation** (`evaluate_greedy`) to assess the agent's true performance without noise and save the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 91220,
     "status": "ok",
     "timestamp": 1741771752738,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "yuzI0m5u5vVf",
    "outputId": "b7a65da4-bcb5-43f4-8eb4-1e86af4e97ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\GitHub\\Artificial_neural_networks_deep_learning---Practica2\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode    1: score -197.62 (epsilon: 1.000, avg(last 50): -197.62, buffer: 81, steps: 81)\n",
      "Episode    2: score -175.13 (epsilon: 1.000, avg(last 50): -186.37, buffer: 192, steps: 192)\n",
      "Episode    3: score -127.37 (epsilon: 1.000, avg(last 50): -166.71, buffer: 272, steps: 272)\n",
      "Episode    4: score  -60.63 (epsilon: 1.000, avg(last 50): -140.19, buffer: 338, steps: 338)\n",
      "Episode    5: score -393.71 (epsilon: 1.000, avg(last 50): -190.89, buffer: 465, steps: 465)\n",
      "Episode    6: score -195.62 (epsilon: 1.000, avg(last 50): -191.68, buffer: 541, steps: 541)\n",
      "Episode    7: score -315.52 (epsilon: 1.000, avg(last 50): -209.37, buffer: 654, steps: 654)\n",
      "Episode    8: score -108.86 (epsilon: 1.000, avg(last 50): -196.81, buffer: 718, steps: 718)\n",
      "Episode    9: score -201.87 (epsilon: 1.000, avg(last 50): -197.37, buffer: 839, steps: 839)\n",
      "Episode   10: score -118.69 (epsilon: 1.000, avg(last 50): -189.5, buffer: 901, steps: 901)\n",
      "Episode   11: score -105.41 (epsilon: 1.000, avg(last 50): -181.86, buffer: 1008, steps: 1008)\n",
      "Episode   12: score -135.68 (epsilon: 1.000, avg(last 50): -178.01, buffer: 1079, steps: 1079)\n",
      "Episode   13: score -229.99 (epsilon: 1.000, avg(last 50): -182.01, buffer: 1163, steps: 1163)\n",
      "Episode   14: score -207.68 (epsilon: 1.000, avg(last 50): -183.84, buffer: 1222, steps: 1222)\n",
      "Episode   15: score -451.96 (epsilon: 1.000, avg(last 50): -201.72, buffer: 1304, steps: 1304)\n",
      "Episode   16: score -144.75 (epsilon: 1.000, avg(last 50): -198.16, buffer: 1382, steps: 1382)\n",
      "Episode   17: score -127.73 (epsilon: 1.000, avg(last 50): -194.01, buffer: 1470, steps: 1470)\n",
      "Episode   18: score -143.82 (epsilon: 1.000, avg(last 50): -191.22, buffer: 1601, steps: 1601)\n",
      "Episode   19: score -275.81 (epsilon: 1.000, avg(last 50): -195.68, buffer: 1708, steps: 1708)\n",
      "Episode   20: score  -13.51 (epsilon: 1.000, avg(last 50): -186.57, buffer: 1794, steps: 1794)\n",
      "Episode   21: score -174.55 (epsilon: 1.000, avg(last 50): -186.0, buffer: 1877, steps: 1877)\n",
      "Episode   22: score -133.27 (epsilon: 1.000, avg(last 50): -183.6, buffer: 1935, steps: 1935)\n",
      "Episode   23: score -133.15 (epsilon: 1.000, avg(last 50): -181.41, buffer: 2026, steps: 2026)\n",
      "Episode   24: score -261.33 (epsilon: 1.000, avg(last 50): -184.74, buffer: 2131, steps: 2131)\n",
      "Episode   25: score -199.44 (epsilon: 1.000, avg(last 50): -185.32, buffer: 2260, steps: 2260)\n",
      "   >> Greedy eval (10 eps): -523.66\n",
      "Episode   26: score -433.82 (epsilon: 1.000, avg(last 50): -194.88, buffer: 2348, steps: 2348)\n",
      "Episode   27: score -287.15 (epsilon: 1.000, avg(last 50): -198.3, buffer: 2441, steps: 2441)\n",
      "Episode   28: score  -84.07 (epsilon: 1.000, avg(last 50): -194.22, buffer: 2510, steps: 2510)\n",
      "Episode   29: score   -7.45 (epsilon: 1.000, avg(last 50): -187.78, buffer: 2615, steps: 2615)\n",
      "Episode   30: score  -99.39 (epsilon: 1.000, avg(last 50): -184.83, buffer: 2705, steps: 2705)\n",
      "Episode   31: score -115.65 (epsilon: 1.000, avg(last 50): -182.6, buffer: 2819, steps: 2819)\n",
      "Episode   32: score -109.79 (epsilon: 1.000, avg(last 50): -180.33, buffer: 2934, steps: 2934)\n",
      "Episode   33: score  -217.5 (epsilon: 1.000, avg(last 50): -181.45, buffer: 3005, steps: 3005)\n",
      "Episode   34: score  -96.58 (epsilon: 1.000, avg(last 50): -178.96, buffer: 3083, steps: 3083)\n",
      "Episode   35: score  -121.3 (epsilon: 1.000, avg(last 50): -177.31, buffer: 3170, steps: 3170)\n",
      "Episode   36: score  -296.9 (epsilon: 1.000, avg(last 50): -180.63, buffer: 3295, steps: 3295)\n",
      "Episode   37: score -103.73 (epsilon: 1.000, avg(last 50): -178.55, buffer: 3351, steps: 3351)\n",
      "Episode   38: score -120.18 (epsilon: 1.000, avg(last 50): -177.02, buffer: 3421, steps: 3421)\n",
      "Episode   39: score -347.76 (epsilon: 1.000, avg(last 50): -181.39, buffer: 3534, steps: 3534)\n",
      "Episode   40: score -146.71 (epsilon: 1.000, avg(last 50): -180.53, buffer: 3589, steps: 3589)\n",
      "Episode   41: score -111.64 (epsilon: 1.000, avg(last 50): -178.85, buffer: 3675, steps: 3675)\n",
      "Episode   42: score -107.08 (epsilon: 1.000, avg(last 50): -177.14, buffer: 3750, steps: 3750)\n",
      "Episode   43: score -443.19 (epsilon: 1.000, avg(last 50): -183.33, buffer: 3835, steps: 3835)\n",
      "Episode   44: score -122.14 (epsilon: 1.000, avg(last 50): -181.94, buffer: 3914, steps: 3914)\n",
      "Episode   45: score -103.04 (epsilon: 1.000, avg(last 50): -180.18, buffer: 3974, steps: 3974)\n",
      "Episode   46: score -130.41 (epsilon: 1.000, avg(last 50): -179.1, buffer: 4048, steps: 4048)\n",
      "Episode   47: score -238.75 (epsilon: 1.000, avg(last 50): -180.37, buffer: 4135, steps: 4135)\n",
      "Episode   48: score  -111.0 (epsilon: 1.000, avg(last 50): -178.92, buffer: 4253, steps: 4253)\n",
      "Episode   49: score  -92.06 (epsilon: 1.000, avg(last 50): -177.15, buffer: 4351, steps: 4351)\n",
      "Episode   50: score  -88.08 (epsilon: 1.000, avg(last 50): -175.37, buffer: 4435, steps: 4435)\n",
      "   >> Greedy eval (10 eps): -646.49\n",
      "Episode   51: score -227.68 (epsilon: 1.000, avg(last 50): -175.97, buffer: 4535, steps: 4535)\n",
      "Episode   52: score  -29.62 (epsilon: 1.000, avg(last 50): -173.06, buffer: 4618, steps: 4618)\n",
      "Episode   53: score -293.99 (epsilon: 1.000, avg(last 50): -176.39, buffer: 4732, steps: 4732)\n",
      "Episode   54: score -353.73 (epsilon: 1.000, avg(last 50): -182.26, buffer: 4809, steps: 4809)\n",
      "Episode   55: score -111.32 (epsilon: 1.000, avg(last 50): -176.61, buffer: 4919, steps: 4919)\n",
      "Episode   56: score -211.78 (epsilon: 1.000, avg(last 50): -176.93, buffer: 5026, steps: 5026)\n",
      "Episode   57: score -231.09 (epsilon: 1.000, avg(last 50): -175.24, buffer: 5119, steps: 5119)\n",
      "Episode   58: score -270.35 (epsilon: 1.000, avg(last 50): -178.47, buffer: 5215, steps: 5215)\n",
      "Episode   59: score   -99.7 (epsilon: 1.000, avg(last 50): -176.43, buffer: 5271, steps: 5271)\n",
      "Episode   60: score  -131.0 (epsilon: 1.000, avg(last 50): -176.68, buffer: 5366, steps: 5366)\n",
      "Episode   61: score -115.72 (epsilon: 1.000, avg(last 50): -176.88, buffer: 5478, steps: 5478)\n",
      "Episode   62: score -119.72 (epsilon: 1.000, avg(last 50): -176.56, buffer: 5573, steps: 5573)\n",
      "Episode   63: score -117.82 (epsilon: 1.000, avg(last 50): -174.32, buffer: 5645, steps: 5645)\n",
      "Episode   64: score -153.07 (epsilon: 1.000, avg(last 50): -173.23, buffer: 5711, steps: 5711)\n",
      "Episode   65: score  -100.9 (epsilon: 1.000, avg(last 50): -166.21, buffer: 5817, steps: 5817)\n",
      "Episode   66: score  -51.43 (epsilon: 1.000, avg(last 50): -164.34, buffer: 5883, steps: 5883)\n",
      "Episode   67: score -123.41 (epsilon: 1.000, avg(last 50): -164.25, buffer: 5951, steps: 5951)\n",
      "Episode   68: score -457.85 (epsilon: 1.000, avg(last 50): -170.53, buffer: 6044, steps: 6044)\n",
      "Episode   69: score -141.62 (epsilon: 1.000, avg(last 50): -167.85, buffer: 6130, steps: 6130)\n",
      "Episode   70: score -494.09 (epsilon: 1.000, avg(last 50): -177.46, buffer: 6214, steps: 6214)\n",
      "Episode   71: score -100.61 (epsilon: 1.000, avg(last 50): -175.98, buffer: 6287, steps: 6287)\n",
      "Episode   72: score  -97.36 (epsilon: 1.000, avg(last 50): -175.26, buffer: 6357, steps: 6357)\n",
      "Episode   73: score  -93.03 (epsilon: 1.000, avg(last 50): -174.46, buffer: 6430, steps: 6430)\n",
      "Episode   74: score -224.45 (epsilon: 1.000, avg(last 50): -173.72, buffer: 6534, steps: 6534)\n",
      "Episode   75: score  -273.9 (epsilon: 1.000, avg(last 50): -175.21, buffer: 6633, steps: 6633)\n",
      "   >> Greedy eval (10 eps): -587.78\n",
      "Episode   76: score -212.84 (epsilon: 1.000, avg(last 50): -170.79, buffer: 6716, steps: 6716)\n",
      "Episode   77: score -142.13 (epsilon: 1.000, avg(last 50): -167.89, buffer: 6799, steps: 6799)\n",
      "Episode   78: score -197.93 (epsilon: 1.000, avg(last 50): -170.17, buffer: 6872, steps: 6872)\n",
      "Episode   79: score -247.85 (epsilon: 1.000, avg(last 50): -174.98, buffer: 6962, steps: 6962)\n",
      "Episode   80: score -131.02 (epsilon: 1.000, avg(last 50): -175.61, buffer: 7029, steps: 7029)\n",
      "Episode   81: score    4.37 (epsilon: 1.000, avg(last 50): -173.21, buffer: 7108, steps: 7108)\n",
      "Episode   82: score -180.89 (epsilon: 1.000, avg(last 50): -174.63, buffer: 7185, steps: 7185)\n",
      "Episode   83: score -261.96 (epsilon: 1.000, avg(last 50): -175.52, buffer: 7293, steps: 7293)\n",
      "Episode   84: score -102.54 (epsilon: 1.000, avg(last 50): -175.64, buffer: 7379, steps: 7379)\n",
      "Episode   85: score -197.04 (epsilon: 1.000, avg(last 50): -177.16, buffer: 7468, steps: 7468)\n",
      "Episode   86: score -311.18 (epsilon: 1.000, avg(last 50): -177.44, buffer: 7576, steps: 7576)\n",
      "Episode   87: score -238.39 (epsilon: 1.000, avg(last 50): -180.13, buffer: 7662, steps: 7662)\n",
      "Episode   88: score -113.38 (epsilon: 1.000, avg(last 50): -180.0, buffer: 7725, steps: 7725)\n",
      "Episode   89: score  -79.39 (epsilon: 1.000, avg(last 50): -174.63, buffer: 7789, steps: 7789)\n",
      "Episode   90: score -433.24 (epsilon: 1.000, avg(last 50): -180.36, buffer: 7897, steps: 7897)\n",
      "Episode   91: score -130.97 (epsilon: 1.000, avg(last 50): -180.75, buffer: 7991, steps: 7991)\n",
      "Episode   92: score -122.34 (epsilon: 1.000, avg(last 50): -181.05, buffer: 8072, steps: 8072)\n",
      "Episode   93: score -290.46 (epsilon: 1.000, avg(last 50): -178.0, buffer: 8177, steps: 8177)\n",
      "Episode   94: score -130.45 (epsilon: 1.000, avg(last 50): -178.16, buffer: 8261, steps: 8261)\n",
      "Episode   95: score   -83.2 (epsilon: 1.000, avg(last 50): -177.77, buffer: 8364, steps: 8364)\n",
      "Episode   96: score  -29.05 (epsilon: 1.000, avg(last 50): -175.74, buffer: 8442, steps: 8442)\n",
      "Episode   97: score  -269.0 (epsilon: 1.000, avg(last 50): -176.35, buffer: 8567, steps: 8567)\n",
      "Episode   98: score -248.26 (epsilon: 1.000, avg(last 50): -179.09, buffer: 8651, steps: 8651)\n",
      "Episode   99: score -416.28 (epsilon: 1.000, avg(last 50): -185.57, buffer: 8749, steps: 8749)\n",
      "Episode  100: score -277.79 (epsilon: 1.000, avg(last 50): -189.37, buffer: 8855, steps: 8855)\n",
      "   >> Greedy eval (10 eps): -450.26\n",
      "Episode  101: score  -204.0 (epsilon: 1.000, avg(last 50): -188.9, buffer: 8944, steps: 8944)\n",
      "Episode  102: score -158.28 (epsilon: 1.000, avg(last 50): -191.47, buffer: 9014, steps: 9014)\n",
      "Episode  103: score -337.93 (epsilon: 1.000, avg(last 50): -192.35, buffer: 9121, steps: 9121)\n",
      "Episode  104: score -187.34 (epsilon: 1.000, avg(last 50): -189.02, buffer: 9202, steps: 9202)\n",
      "Episode  105: score   48.49 (epsilon: 1.000, avg(last 50): -185.82, buffer: 9273, steps: 9273)\n",
      "Episode  106: score -153.27 (epsilon: 1.000, avg(last 50): -184.65, buffer: 9344, steps: 9344)\n",
      "Episode  107: score -339.92 (epsilon: 1.000, avg(last 50): -186.83, buffer: 9439, steps: 9439)\n",
      "Episode  108: score -270.58 (epsilon: 1.000, avg(last 50): -186.83, buffer: 9543, steps: 9543)\n",
      "Episode  109: score -141.93 (epsilon: 1.000, avg(last 50): -187.68, buffer: 9625, steps: 9625)\n",
      "Episode  110: score -301.98 (epsilon: 1.000, avg(last 50): -191.1, buffer: 9742, steps: 9742)\n",
      "Episode  111: score -171.74 (epsilon: 1.000, avg(last 50): -192.22, buffer: 9852, steps: 9852)\n",
      "Episode  112: score -313.35 (epsilon: 1.000, avg(last 50): -196.09, buffer: 9939, steps: 9939)\n",
      "Episode  113: score -362.32 (epsilon: 1.000, avg(last 50): -200.98, buffer: 10024, steps: 10024)\n",
      "Episode  114: score -173.78 (epsilon: 0.999, avg(last 50): -201.4, buffer: 10143, steps: 10143)\n",
      "Episode  115: score -305.66 (epsilon: 0.998, avg(last 50): -205.49, buffer: 10315, steps: 10315)\n",
      "Episode  116: score  -54.88 (epsilon: 0.997, avg(last 50): -205.56, buffer: 10388, steps: 10388)\n",
      "Episode  117: score -132.15 (epsilon: 0.997, avg(last 50): -205.73, buffer: 10473, steps: 10473)\n",
      "Episode  118: score -102.55 (epsilon: 0.996, avg(last 50): -198.63, buffer: 10566, steps: 10566)\n",
      "Episode  119: score -348.57 (epsilon: 0.996, avg(last 50): -202.77, buffer: 10657, steps: 10657)\n",
      "Episode  120: score -372.41 (epsilon: 0.995, avg(last 50): -200.33, buffer: 10776, steps: 10776)\n",
      "Episode  121: score -106.17 (epsilon: 0.994, avg(last 50): -200.45, buffer: 10863, steps: 10863)\n",
      "Episode  122: score -363.94 (epsilon: 0.994, avg(last 50): -205.78, buffer: 10943, steps: 10943)\n",
      "Episode  123: score -117.07 (epsilon: 0.993, avg(last 50): -206.26, buffer: 11033, steps: 11033)\n",
      "Episode  124: score -380.94 (epsilon: 0.993, avg(last 50): -209.39, buffer: 11136, steps: 11136)\n",
      "Episode  125: score  -79.37 (epsilon: 0.992, avg(last 50): -205.5, buffer: 11207, steps: 11207)\n",
      "   >> Greedy eval (10 eps): -169.16\n",
      "Episode  126: score -127.68 (epsilon: 0.992, avg(last 50): -203.79, buffer: 11278, steps: 11278)\n",
      "Episode  127: score  -72.97 (epsilon: 0.991, avg(last 50): -202.41, buffer: 11360, steps: 11360)\n",
      "Episode  128: score -336.23 (epsilon: 0.991, avg(last 50): -205.18, buffer: 11437, steps: 11437)\n",
      "Episode  129: score  -74.28 (epsilon: 0.990, avg(last 50): -201.71, buffer: 11512, steps: 11512)\n",
      "Episode  130: score  -78.84 (epsilon: 0.990, avg(last 50): -200.66, buffer: 11605, steps: 11605)\n",
      "Episode  131: score -136.88 (epsilon: 0.989, avg(last 50): -203.49, buffer: 11669, steps: 11669)\n",
      "Episode  132: score -316.19 (epsilon: 0.989, avg(last 50): -206.19, buffer: 11743, steps: 11743)\n",
      "Episode  133: score -169.17 (epsilon: 0.988, avg(last 50): -204.34, buffer: 11861, steps: 11861)\n",
      "Episode  134: score  -69.11 (epsilon: 0.987, avg(last 50): -203.67, buffer: 11929, steps: 11929)\n",
      "Episode  135: score -205.71 (epsilon: 0.987, avg(last 50): -203.84, buffer: 12005, steps: 12005)\n",
      "Episode  136: score -135.96 (epsilon: 0.986, avg(last 50): -200.34, buffer: 12132, steps: 12132)\n",
      "Episode  137: score -133.62 (epsilon: 0.986, avg(last 50): -198.24, buffer: 12215, steps: 12215)\n",
      "Episode  138: score -240.19 (epsilon: 0.985, avg(last 50): -200.78, buffer: 12329, steps: 12329)\n",
      "Episode  139: score -212.95 (epsilon: 0.984, avg(last 50): -203.45, buffer: 12443, steps: 12443)\n",
      "Episode  140: score  -83.48 (epsilon: 0.983, avg(last 50): -196.45, buffer: 12549, steps: 12549)\n",
      "Episode  141: score -230.12 (epsilon: 0.983, avg(last 50): -198.44, buffer: 12657, steps: 12657)\n",
      "Episode  142: score -109.59 (epsilon: 0.982, avg(last 50): -198.18, buffer: 12732, steps: 12732)\n",
      "Episode  143: score  -79.14 (epsilon: 0.982, avg(last 50): -193.96, buffer: 12797, steps: 12797)\n",
      "Episode  144: score -124.19 (epsilon: 0.981, avg(last 50): -193.83, buffer: 12885, steps: 12885)\n",
      "Episode  145: score -154.88 (epsilon: 0.980, avg(last 50): -195.26, buffer: 13025, steps: 13025)\n",
      "Episode  146: score -231.81 (epsilon: 0.979, avg(last 50): -199.32, buffer: 13148, steps: 13148)\n",
      "Episode  147: score  -188.3 (epsilon: 0.979, avg(last 50): -197.71, buffer: 13226, steps: 13226)\n",
      "Episode  148: score   11.98 (epsilon: 0.978, avg(last 50): -192.5, buffer: 13352, steps: 13352)\n",
      "Episode  149: score  -91.36 (epsilon: 0.977, avg(last 50): -186.0, buffer: 13463, steps: 13463)\n",
      "Episode  150: score -109.13 (epsilon: 0.977, avg(last 50): -182.63, buffer: 13539, steps: 13539)\n",
      "   >> Greedy eval (10 eps): -153.93\n",
      "Episode  151: score -120.36 (epsilon: 0.976, avg(last 50): -180.96, buffer: 13637, steps: 13637)\n",
      "Episode  152: score -285.52 (epsilon: 0.975, avg(last 50): -183.5, buffer: 13758, steps: 13758)\n",
      "Episode  153: score -300.36 (epsilon: 0.975, avg(last 50): -182.75, buffer: 13852, steps: 13852)\n",
      "Episode  154: score -168.37 (epsilon: 0.974, avg(last 50): -182.37, buffer: 13950, steps: 13950)\n",
      "Episode  155: score  -85.83 (epsilon: 0.974, avg(last 50): -185.06, buffer: 14014, steps: 14014)\n",
      "Episode  156: score -310.11 (epsilon: 0.973, avg(last 50): -188.19, buffer: 14145, steps: 14145)\n",
      "Episode  157: score -299.72 (epsilon: 0.972, avg(last 50): -187.39, buffer: 14259, steps: 14259)\n",
      "Episode  158: score -217.98 (epsilon: 0.971, avg(last 50): -186.34, buffer: 14368, steps: 14368)\n",
      "Episode  159: score -163.06 (epsilon: 0.971, avg(last 50): -186.76, buffer: 14427, steps: 14427)\n",
      "Episode  160: score -108.42 (epsilon: 0.971, avg(last 50): -182.89, buffer: 14508, steps: 14508)\n",
      "Episode  161: score -168.32 (epsilon: 0.970, avg(last 50): -182.82, buffer: 14620, steps: 14620)\n",
      "Episode  162: score -340.73 (epsilon: 0.969, avg(last 50): -183.37, buffer: 14703, steps: 14703)\n",
      "Episode  163: score  -59.77 (epsilon: 0.968, avg(last 50): -177.32, buffer: 14832, steps: 14832)\n",
      "Episode  164: score  -92.75 (epsilon: 0.968, avg(last 50): -175.7, buffer: 14904, steps: 14904)\n",
      "Episode  165: score  -78.33 (epsilon: 0.967, avg(last 50): -171.15, buffer: 14991, steps: 14991)\n",
      "Episode  166: score -131.34 (epsilon: 0.967, avg(last 50): -172.68, buffer: 15060, steps: 15060)\n",
      "Episode  167: score -125.96 (epsilon: 0.966, avg(last 50): -172.56, buffer: 15132, steps: 15132)\n",
      "Episode  168: score -307.33 (epsilon: 0.966, avg(last 50): -176.65, buffer: 15235, steps: 15235)\n",
      "Episode  169: score -155.46 (epsilon: 0.965, avg(last 50): -172.79, buffer: 15332, steps: 15332)\n",
      "Episode  170: score -123.06 (epsilon: 0.965, avg(last 50): -167.8, buffer: 15424, steps: 15424)\n",
      "Episode  171: score -303.16 (epsilon: 0.964, avg(last 50): -171.74, buffer: 15535, steps: 15535)\n",
      "Episode  172: score -181.43 (epsilon: 0.963, avg(last 50): -168.09, buffer: 15638, steps: 15638)\n",
      "Episode  173: score -165.92 (epsilon: 0.963, avg(last 50): -169.07, buffer: 15724, steps: 15724)\n",
      "Episode  174: score -202.21 (epsilon: 0.962, avg(last 50): -165.49, buffer: 15820, steps: 15820)\n",
      "Episode  175: score -166.94 (epsilon: 0.961, avg(last 50): -167.25, buffer: 15911, steps: 15911)\n"
     ]
    }
   ],
   "source": [
    "environment, number_of_observations, number_of_actions = create_environment()\n",
    "agent = DQN(number_of_observations, number_of_actions)\n",
    "\n",
    "episode = 0\n",
    "start_time = time.perf_counter()\n",
    "total_steps = 0\n",
    "goal_reached = False\n",
    "\n",
    "def epsilon_by_step(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return 1.0\n",
    "    step2 = step - WARMUP_STEPS\n",
    "    decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY_STEPS\n",
    "    return max(EPSILON_END, EPSILON_START - decay * step2)\n",
    "\n",
    "def evaluate_greedy(agent, env, n_episodes=10):\n",
    "    scores = []\n",
    "    for i in range(n_episodes):\n",
    "        s, _ = env.reset()  # sin seed fijo\n",
    "        done = False\n",
    "        total = 0.0\n",
    "        while not done:\n",
    "            a = agent.select_greedy_policy(s)\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            total += r\n",
    "        scores.append(total)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "\n",
    "while (episode < MAX_EPISODES_FOR_TRAINING) and not(goal_reached):\n",
    "    episode += 1\n",
    "    score = 0.0\n",
    "    state, info = environment.reset(seed=SEED + episode)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        exploration_rate = epsilon_by_step(total_steps)\n",
    "        action = agent.select(state, exploration_rate)\n",
    "\n",
    "        state_next, reward, terminated, truncated, info = environment.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        done_for_target = terminated\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        agent.remember(state, action, reward, state_next, done_for_target)\n",
    "        score += reward\n",
    "\n",
    "        if (total_steps >= WARMUP_STEPS) and (total_steps % TRAIN_EVERY == 0):\n",
    "            agent.learn()\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "    agent.add_score(score)\n",
    "    average_score = agent.average_score(EPISODES_TO_CHECK_TRAINING_GOAL)\n",
    "\n",
    "    print(\"Episode {0:>4}: \".format(episode), end='')\n",
    "    print(\"score {0:>7} \".format(round(score, 2)), end='')\n",
    "    print(\"(epsilon: %.3f, \" % epsilon_by_step(total_steps), end='')\n",
    "    print(\"avg(last {0}): {1:>6}, \".format(EPISODES_TO_CHECK_TRAINING_GOAL, round(average_score, 2)), end='')\n",
    "    print(\"buffer: \" + str(agent.memory.current_size) + \", steps: \" + str(total_steps) + \")\")\n",
    "\n",
    "\n",
    "    if episode % 25 == 0:\n",
    "        greedy_mean = evaluate_greedy(agent, environment, n_episodes=10)\n",
    "        print(f\"   >> Greedy eval (10 eps): {greedy_mean:.2f}\")\n",
    "\n",
    "        if greedy_mean > getattr(agent, \"best_greedy\", -1e9):\n",
    "            agent.best_greedy = greedy_mean\n",
    "            agent.model.save(\"best_model.keras\")\n",
    "\n",
    "        # criterio de parada REAL (el que importa para la práctica)\n",
    "        if greedy_mean >= TRAINING_GOAL:\n",
    "            print(\"✅ Stop: greedy_mean >= 200\")\n",
    "            goal_reached = True\n",
    "\n",
    "\n",
    "    if (episode >= EPISODES_TO_CHECK_TRAINING_GOAL) and (average_score >= TRAINING_GOAL):\n",
    "        goal_reached = True\n",
    "\n",
    "print(\"Time for training: \", end='')\n",
    "print_time(time.perf_counter() - start_time)\n",
    "print(\"Total steps: \", total_steps)\n",
    "print(\"Score (average last episodes):\", round(average_score, 2))\n",
    "print(\"Score (max):\", round(max(agent.scores), 2))\n",
    "\n",
    "agent.display_scores_graphically()\n",
    "agent.model.save(\"last_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSteJT6ULQVG"
   },
   "source": [
    "## Testing program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the **Greedy Policy** for the testing block to evaluate the agent (`agent.select_greedy_policy`), which selects the max Q-value without random noise. This method also applies the necessary state normalization to inputs before passing them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41809,
     "status": "ok",
     "timestamp": 1741771908239,
     "user": {
      "displayName": "Martin Molina Gonzalez",
      "userId": "08504696337223407114"
     },
     "user_tz": -60
    },
    "id": "gIP0LQHGLZPj",
    "outputId": "7e94342e-00d7-408b-90d2-61066f33d267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1: score 266 \n",
      "Episode   2: score 285 \n",
      "Episode   3: score 262 \n",
      "Episode   4: score 273 \n",
      "Episode   5: score 258 \n",
      "Episode   6: score 270 \n",
      "Episode   7: score 275 \n",
      "Episode   8: score 270 \n",
      "Episode   9: score 221 \n",
      "Episode  10: score 244 \n",
      "Episode  11: score 260 \n",
      "Episode  12: score 275 \n",
      "Episode  13: score 262 \n",
      "Episode  14: score 288 \n",
      "Episode  15: score 301 \n",
      "Episode  16: score 278 \n",
      "Episode  17: score 283 \n",
      "Episode  18: score 251 \n",
      "Episode  19: score 217 \n",
      "Episode  20: score 163 \n",
      "Episode  21: score 243 \n",
      "Episode  22: score 250 \n",
      "Episode  23: score 109 \n",
      "Episode  24: score 272 \n",
      "Episode  25: score 272 \n",
      "Episode  26: score 311 \n",
      "Episode  27: score 237 \n",
      "Episode  28: score 279 \n",
      "Episode  29: score 247 \n",
      "Episode  30: score 125 \n",
      "Time for testing: 49 seconds\n",
      "Score (average): 252\n",
      "Score (max): 311\n"
     ]
    }
   ],
   "source": [
    "agent.delete_scores()\n",
    "episode = 0\n",
    "start_time = time.perf_counter()\n",
    "while (episode < EPISODES_TO_EVALUATE_MODEL_PERFORMANCE):\n",
    "    episode += 1\n",
    "    score = 0\n",
    "    state, info = environment.reset()\n",
    "    end_episode = False\n",
    "    while not(end_episode):\n",
    "        # Select an action for the current state\n",
    "        action = agent.select_greedy_policy(state)\n",
    "\n",
    "        # Execute the action in the environment\n",
    "        state_next, reward, terminal_state, truncated, info = environment.step(action)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # Detect end of episode and print\n",
    "        if terminal_state or truncated:\n",
    "            agent.add_score(score)\n",
    "            print(\"Episode {0:>3}: \".format(episode), end = '')\n",
    "            print(\"score {0:>3} \\n\".format(round(score)), end = '')\n",
    "            end_episode = True\n",
    "        else:\n",
    "            state = state_next\n",
    "\n",
    "print(\"Time for testing: \", end = '')\n",
    "print_time(time.perf_counter() - start_time)\n",
    "print(\"Score (average):\", round(np.mean(agent.scores)))\n",
    "print(\"Score (max):\", round(max(agent.scores)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
